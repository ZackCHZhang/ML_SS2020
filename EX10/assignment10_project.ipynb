{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 10: X-Ray Pneumonia Detection\n",
    "\n",
    "In this assignment, you are tasked with developing your own classifier for pneumonia in X-ray images. You will go through to the complete ML development cycle from loading and preprocessing your data to evaluating your models.\n",
    "\n",
    "Download and extract the X-Ray dataset from Ilias to the same directory as your Jupyter notebook. The data is already split into a training, validation, and testing set. The dataset originates from the paper [Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning\n",
    "](https://www.cell.com/cell/fulltext/S0092-8674(18)30154-5).\n",
    "\n",
    "You may use any packages, we encountered during the exercises (numpy, matplotlib, scikit-learn, scikit-image, pandas, pytorch) as well as the Python standard library.\n",
    "\n",
    "You should (at least) address the following points in your development process:\n",
    "\n",
    "- The dataset is imbalanced. Do at least one of the following:\n",
    "    - Augment your dataset by including rotated, flipped, or brightened images. This will also improve the generalization capabilities of your model.\n",
    "    - or: Modify your objective function by weighting the classes differently.\n",
    "- Optimize the hyperparameters of your models using grid-search or random-search on the validation set.\n",
    "- Consider at least two classes of models, e.g. CNN and SVM. At least one of your model classes should be some type of neural network implemented in PyTorch.\n",
    "- After the hyperparameter optimization, select the best-performing models of each class. Evaluate these models on the testing data and visualize your results.\n",
    "\n",
    "\n",
    "*Note*: You have 2 weeks to complete this assignment. The assignment is optional for B.Sc.Data Science students, who have an alternative easier assignment 10 on clustering. If you already have successfully submitted 7 assignments, you do not need to submit this assignment.\n",
    "Presenting this assignment in a video counts as **2 video presentations**. Please indicate if you wish to present at the top of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Put your names here*\n",
    "\n",
    "**Indicate here whether you want to present.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import sklearn\n",
    "import os\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "0%|          | 0/1341 [00:00<?, ?it/s]load images:\n100%|██████████| 1341/1341 [06:39<00:00,  3.36it/s]\n100%|██████████| 3875/3875 [06:51<00:00,  9.41it/s]\n100%|██████████| 2534/2534 [00:05<00:00, 450.73it/s]\n  0%|          | 0/8 [00:00<?, ?it/s]load images:\n100%|██████████| 8/8 [00:01<00:00,  5.14it/s]\n100%|██████████| 8/8 [00:00<00:00, 10.43it/s]\n  0%|          | 0/234 [00:00<?, ?it/s]load images:\n100%|██████████| 234/234 [01:19<00:00,  2.96it/s]\n100%|██████████| 390/390 [00:33<00:00, 11.82it/s]\n100%|██████████| 156/156 [00:00<00:00, 460.18it/s]\n"
    }
   ],
   "source": [
    "# make data loader in this cell\n",
    "# print(os.listdir(os.path.join('chest_xray/train','NORMAL')))\n",
    "def load_data(directory):\n",
    "    print(\"load images:\")\n",
    "    label=[]\n",
    "    normal_images=[]\n",
    "    pneumonia_images=[]\n",
    "    # subFolder= ['NORMAL','PNEUMONIA']\n",
    "    # for d in subFolder:\n",
    "    directories=[d for d in os.listdir(os.path.join(directory,'NORMAL'))]\n",
    "    for f in tqdm(directories,position=0):\n",
    "        I=skimage.io.imread(os.path.join(os.path.join(directory,'NORMAL'),f))\n",
    "        if len(I.shape) ==2:\n",
    "            I=skimage.color.gray2rgb(I)\n",
    "        normal_images.append(skimage.transform.resize(I,(128,128)))\n",
    "        label.append(0)\n",
    "    directories=[d for d in os.listdir(os.path.join(directory,'PNEUMONIA'))]\n",
    "    for f in tqdm(directories,position=0):\n",
    "        I=skimage.io.imread(os.path.join(os.path.join(directory,'PNEUMONIA'),f))\n",
    "        if len(I.shape)==2:\n",
    "            I=skimage.color.gray2rgb(I)\n",
    "        pneumonia_images.append(skimage.transform.resize(I,(128,128)))\n",
    "        label.append(1)\n",
    "\n",
    "    \n",
    "    anglelist=[90,270,180]\n",
    "\n",
    "    if len(normal_images)<len(pneumonia_images):\n",
    "        image_num_diff=len(pneumonia_images)-len(normal_images)\n",
    "        for i in tqdm(range(image_num_diff)):\n",
    "            idx=random.randint(0,len(normal_images)-1)\n",
    "            angle_idx=random.randint(0,2)\n",
    "            # angle=random.randint(-90,90)\n",
    "            normal_images.append(skimage.transform.rotate(normal_images[idx],anglelist[angle_idx] ))\n",
    "            label.append(0)\n",
    "    elif len(normal_images)>len(pneumonia_images):\n",
    "        image_num_diff=-len(pneumonia_images)+len(normal_images)\n",
    "        for i in range(image_num_diff):\n",
    "            idx=random.randint(0,len(pneumonia_images-1))\n",
    "            angle_idx=random.randint(0,2)\n",
    "            # angle=random.randint(-90,90)\n",
    "            pneumonia_images.append(skimage.transform.rotate(pneumonia_images[idx],anglelist[angle_idx]) )\n",
    "            label.append(1)\n",
    "    \n",
    "    images=normal_images+pneumonia_images\n",
    "    images,label=np.array(images),np.array(label)\n",
    "    return images,label\n",
    "\n",
    "\n",
    "X_train,y_train= load_data('chest_xray/train')\n",
    "X_val,y_val,=load_data('chest_xray/val')\n",
    "X_test,y_test=load_data('chest_xray/test')\n",
    "\n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(128, 128, 3)\n"
    }
   ],
   "source": [
    "# test the dataset and define dateloader\n",
    "device=torch.device('cuda')\n",
    "print(X_train[0].shape)\n",
    "class BasicDataset(object):\n",
    "    def __init__(self, X, y,transform):\n",
    "        self.X = np.moveaxis(X,-1,1)\n",
    "        self.y = y\n",
    "        self.transform=transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.transform(torch.Tensor(self.X[idx])).numpy(), y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the nn by pytorch\n",
    "class RNNClassifer(nn.Module):\n",
    "    def __init__(self,hidden_size):\n",
    "        super(RNNClassifer,self).__init__()\n",
    "        self.cnnLayer1=nn.Sequential(\n",
    "            nn.Conv2d(3,16,3,2,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16,16,3,2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.cnnLayer2=nn.Sequential(\n",
    "            nn.Conv2d(16,32,3,1,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,32,3,1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.cnnLayer3=nn.Sequential(\n",
    "            nn.Conv2d(32,48,3,1,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(48,48,3,1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.shortcut_cnn1=nn.Sequential(\n",
    "            nn.Conv2d(3,16,1,4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.shortcut_cnn2=nn.Sequential(\n",
    "            nn.Conv2d(16,32,1,1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.shortcut_cnn3=nn.Sequential(\n",
    "            nn.Conv2d(32,48,1,1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.poollayer=nn.MaxPool2d(3,2)\n",
    "        self.gaplayeer=nn.AvgPool2d(3,2)\n",
    "        self.dense_layer=nn.Sequential(\n",
    "            nn.Linear(48*1*1,hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.output_layer=nn.Linear(hidden_size,2)\n",
    "    def forward(self,X):\n",
    "        cnn_out=self.cnnLayer1(X)\n",
    "        shortcut_out=self.shortcut_cnn1(X)\n",
    "\n",
    "        out=self.poollayer(cnn_out+shortcut_out)\n",
    "\n",
    "        cnn_out=self.cnnLayer2(out)\n",
    "        shortcut_out=self.shortcut_cnn2(out)\n",
    "        out=self.poollayer(cnn_out+shortcut_out)\n",
    "\n",
    "        cnn_out=self.cnnLayer3(out)\n",
    "        shortcut_out=self.shortcut_cnn3(out)\n",
    "        out=self.poollayer(cnn_out+shortcut_out)\n",
    "\n",
    "        out=self.gaplayeer(out)\n",
    "\n",
    "        # print(out.shape)\n",
    "        out=out.view(-1,48*1*1)\n",
    "        out=self.dense_layer(out)\n",
    "        out=self.output_layer(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "def train_model(model,dataset,learing_rate,batch_size,epochs):\n",
    "    optimizer=optim.Adam(model.parameters(),lr=learing_rate)\n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    for epoch in range(epochs):\n",
    "        data_loader=data.DataLoader(dataset=dataset,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "        epoch_loss=0.0\n",
    "        for batch in tqdm(data_loader,position=0):\n",
    "            model.zero_grad()\n",
    "            model.zero_grad()\n",
    "\n",
    "            yhat=model.forward(batch['X'].float().to(device))\n",
    "\n",
    "            batch_loss=F.cross_entropy(yhat,batch['y'].long().to(device))\n",
    "\n",
    "            epoch_loss+=batch_loss.item()\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss}')\n",
    "        loss_list.append(epoch_loss)\n",
    "        acc_list.append(evaluate_model)\n",
    "    return acc_list,loss_list\n",
    "\n",
    "def evaluate_model(model, dataset_val):\n",
    "    with torch.no_grad():\n",
    "        X=torch.from_numpy(np.array([sample['X'] for sample in dataset_val])).float()\n",
    "        yhat_unnormalized=model.forward(X.to(device)).cpu().detach().numpy()\n",
    "    \n",
    "    yhat=np.argmax(yhat_unnormalized, axis=1)\n",
    "    y=np.array([sample['y'] for sample in dataset_val])\n",
    "    acc=accuracy_score(yhat,y)\n",
    "    # print(yhat_unnormalized)\n",
    "\n",
    "    print(\"validation acc is\",acc)\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    # torchvision.transforms.Normalize(mean=0,std=0.1),\n",
    "    torchvision.transforms.ColorJitter(),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor()\n",
    "\n",
    "])\n",
    "train_dataset=BasicDataset(X_train,y_train,transform)\n",
    "val_dataset=BasicDataset(X_val,y_val,transform)\n",
    "test_dataset=BasicDataset(X_test,y_test,transform)\n",
    "epochs=20\n",
    "MAX_EVALS=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 60/60 [00:06<00:00,  9.01it/s]\nEpoch 1/20 - Loss: 1586827596.0497715\n100%|██████████| 60/60 [00:06<00:00,  9.09it/s]\nEpoch 2/20 - Loss: 83.15593415498734\n100%|██████████| 60/60 [00:06<00:00,  8.83it/s]\nEpoch 3/20 - Loss: 41.65532624721527\n 62%|██████▏   | 37/60 [00:04<00:02,  8.24it/s]"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-8a2ad910ec3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRNNClassifer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0macc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-6243126ae1cf>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, dataset, learing_rate, batch_size, epochs)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mdata_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop_last\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mepoch_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    935\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m    936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 937\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    938\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-742359280b9c>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_grid={\n",
    "    'learning_rate':list(np.logspace(np.log10(0.005),np.log10(0.5),base=10,num=1000)),\n",
    "    'batch_size':[16,32,64,128,256],\n",
    "    'hidden_size':[64,128,256,512,1024]\n",
    "    }\n",
    "\n",
    "\n",
    "best_acc=0\n",
    "best_hypterparameter={}\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for i in range(MAX_EVALS):\n",
    "    hypterparameter={k:random.sample(v,1)[0] for k,v in param_grid.items()}\n",
    "\n",
    "    lr=hypterparameter['learning_rate']\n",
    "    batch_size=hypterparameter['batch_size']\n",
    "    hidden_size=hypterparameter['hidden_size']\n",
    "\n",
    "    model=RNNClassifer(hidden_size).to(device=device)\n",
    "    model.train()\n",
    "    train_model(model,train_dataset,lr,batch_size,epochs)\n",
    "    model.eval()\n",
    "    acc=evaluate_model(model,test_dataset)\n",
    "    if acc>best_acc:\n",
    "        best_hypterparameter=hypterparameter\n",
    "        best_acc=acc\n",
    "        torch.save(model.state_dict(),\"best_sol.pt\")\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"best situation Hyperparameter :\")\n",
    "print(best_hypterparameter)\n",
    "\n",
    "loss_list=[]\n",
    "acc_list=[]\n",
    "\n",
    "best_model=RNNClassifer(best_hypterparameter['hidden_size']).to(device)\n",
    "best_model.load_state_dict(torch.load(\"best_sol.pt\"))\n",
    "best_model.train()\n",
    "acc_list,loss_list=train_model(best_model,train_dataset,best_hypterparameter['learning_rate'],best_hypterparameter['batch_size'],200) \n",
    "best_model.eval()\n",
    "acc=evaluate_model(best_model,test_dataset)\n",
    "print(\"____________________________________\")\n",
    "print(\"The final accuracy is:\",acc)\n",
    "print(\"____________________________________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_label=range(0,200)\n",
    "plt.plot(x_label,loss_list,color='green')\n",
    "plt.show()\n",
    "plt.plot(x_label,acc_list,color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_model=torch.load(\"best_sol.pt\")\n",
    "# print(best_model)\n",
    "best_model=RNNClassifer(128).to(device)\n",
    "# best_model.load_state_dict(torch.load(\"best_sol.pt\"))\n",
    "train_model(best_model,train_dataset,0.007,256,20)\n",
    "acc=evaluate_model(best_model,test_dataset)\n",
    "print(\"____________________________________\")\n",
    "print(\"The final accuracy is:\",acc)\n",
    "print(\"____________________________________\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svcclassfer=SVC(kernel='rbf')\n",
    "X_train.tolist()\n",
    "y_train.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "1%|          | 67/7750 [00:00<00:11, 663.37it/s](128, 128, 3)\n100%|██████████| 7750/7750 [00:05<00:00, 1521.10it/s]\n"
    }
   ],
   "source": [
    "print(X_train[0].shape)\n",
    "def trans_dataset_gray(data):\n",
    "    res=[]\n",
    "    trans=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            torchvision.transforms.Grayscale(),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "    for iter in tqdm(data):\n",
    "        temp=trans(torch.Tensor(iter)).numpy()\n",
    "        \n",
    "        res.append(np.array(temp.flatten()))\n",
    "    return res\n",
    "X_train_SVM=trans_dataset_gray(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(384,)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "print(X_train_SVM[0].shape)\n",
    "svcclassfer.fit(X_train_SVM,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 780/780 [00:00<00:00, 2247.85it/s]\n0.6333333333333333\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'              precision    recall  f1-score   support\\n\\n           0       0.94      0.58      0.72       626\\n           1       0.33      0.84      0.47       154\\n\\n    accuracy                           0.63       780\\n   macro avg       0.63      0.71      0.60       780\\nweighted avg       0.82      0.63      0.67       780\\n'"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "X_test_SVM=trans_dataset_gray(X_test)\n",
    "acc=accuracy_score(svcclassfer.predict(X_test_SVM),y_test)\n",
    "print(acc)\n",
    "classification_report(svcclassfer.predict(X_test_SVM),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('69566': virtualenv)",
   "language": "python",
   "name": "python_defaultSpec_1595154458838"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}